# -*- coding: utf-8 -*-
"""Copy of PM

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1poF7rsCwomk6SxJSVlBXvAcAU6R47cE-

First extract data for some famous indicies, like S&P 500:

S&P 500 goes back to 1957, GLD stock stated in 2004.
Gold, Muhlenkamp argues, has been the better portfolio hedge than Treasurys since Covid.
“Nobody’s buying more [gold] mines because too many investors got burned by miners last time when prices fell,” he says, noting that miners tend to trade more volatility than the price of gold.

Precious metal miners, as measured by the FTSE Gold Mines Index, gained a remarkable 15.85 percent in the three months ended December 31. Among the leaders in 2018 were Nevsun Resources, up 106 percent for the 12-month period; Kirkland Lake Gold, up 81 percent; SSR Mining, up 45 percent; and North American Palladium, up 38 percent.

The S&P 500 Stock Index is a widely recognized capitalization-weighted index of 500 common stock prices in U.S. companies. The FTSE Gold Mines Index encompasses all gold mining companies that have a sustainable and attributable gold production of at least 300,000 ounces a year, and that derive 75% or more of their revenue from mined gold.

SPDR Gold Trust (GLDSPDR Gold Shares0.0%) is the oldest gold exchange-traded fund trading on the U.S. market. Its goal is to reflect the price of gold bullion, which is the physical form of pure gold, typically in bars or coins.
"""

# Cell 0: fetch historical price data for indices / ETFs
import yfinance as yf
import datetime
import pandas as pd
import math
import numpy as np
import matplotlib.pyplot as plt



# example tickers: S&P 500 (^GSPC) and GLD (gold ETF)
tickers = ["^GSPC", "GLD"]
start_date = datetime.datetime(2004, 1, 1)
end_date = datetime.datetime(2025, 11, 20)

# fetch 5 years of daily data
data = {}
for t in tickers:
    tk = yf.Ticker(t)
    df = tk.history(start=start_date, end=end_date)   # daily OHLCV with DatetimeIndex
    # keep only adjusted close for returns analysis
    df = df[['Close','Open', 'Volume','Dividends','Stock Splits']].rename(columns={'Close':'Close_'+t})
    data[t] = df

# join into one DataFrame of close prices
closes = pd.concat([data[t]['Close_'+t] for t in tickers], axis=1)
closes.index = pd.to_datetime(closes.index)

# print("Price head:")
print(closes.head())
returns = closes.pct_change()

"""Checking for missing values, and summary statistics.

# Cell 2: returns and summary statistics
"""

# Cell 5: a quick momentum strategy example (1-month signal)
# long if 1-month return > 0 else cash (0)
# Cell 4: simple equal-weighted portfolio and metrics
weights = np.array([0.5, 0.5])  # adjust to match len(tickers)
if len(weights) != len(returns.columns):
    raise ValueError("weights length must match number of tickers")

# portfolio daily returns
port_daily = returns.dot(weights)
port_cum = (1 + port_daily).cumprod() - 1
cum_returns = (1 + returns).cumprod() - 1
lookback = 21  # ~1 month
signal = returns.rolling(window=lookback).sum()  # rolling sum ≈ multi-day return
# build a portfolio that is long S&P when S&P momentum > 0, otherwise hold GLD
# (this is illustrative; adapt logic to your strategy)
signal_s = signal['Close_^GSPC']  # adjust column name exactly
position = (signal_s > 0).astype(int)  # 1 when momentum positive, 0 otherwise
# backtest: daily strategy returns = position.shift(1) * S&P returns + (1-position.shift(1)) * GLD returns
strat_returns = position.shift(1).fillna(0) * returns['Close_^GSPC'] + (1 - position.shift(1).fillna(0)) * returns['Close_GLD']
strat_cum = (1 + strat_returns).cumprod() - 1

plt.figure(figsize=(10,5))
plt.plot(strat_cum.index, strat_cum, label="Simple momentum strategy")
plt.plot(port_cum.index, port_cum, label="Equal-weighted portfolio")

plt.title("Strategy vs Benchmark")
plt.legend()
plt.show()

# If yesterday’s signal was 1 → today’s return = S&P return.

# If yesterday’s signal was 0 → today’s return = GLD return.
import yfinance as yf, pandas as pd, numpy as np
import matplotlib.pyplot as plt
from scipy import stats
from datetime import datetime, timedelta
from dateutil.relativedelta import relativedelta

def fetch_closes(tickers, start, end):
  data = {}
  for t in tickers:
      tk = yf.Ticker(t)
      df = tk.history(start=start, end=end, auto_adjust=True)   # daily OHLCV with DatetimeIndex
      # keep only adjusted close for returns analysis
      df = df[['Close','Volume']].rename(columns={'Close':'Close_'+t})
      df_clean = df[df["Volume"] > 0].copy()
      data[t] = df_clean
      # join into one DataFrame of close prices
  closes = pd.concat([data[t]['Close_'+t] for t in tickers], axis=1)
  closes.index = pd.to_datetime(closes.index)
  return(closes)

def fetch_closes_sum(tickers,weights, start, end):
  data = {}
  for t in tickers:
      tk = yf.Ticker(t)
      df = tk.history(start=start, end=end, auto_adjust=True)   # daily OHLCV with DatetimeIndex
      # keep only adjusted close for returns analysis
      df = df[['Close','Volume']].rename(columns={'Close':'Close_'+t})
      df_clean = df[df["Volume"] > 0].copy()
      data[t] = df_clean
      # join into one DataFrame of close prices
  closes = pd.concat([data[t]['Close_'+t] for t in tickers], axis=1)
  closes = closes.dot(weights)
  closes.index = pd.to_datetime(closes.index)
  return(closes)

# print(fetch_closes_sum(['AAPL', 'GOOG'], [0.8, 0.2], start = "2025-1-10", end = "2025-3-10"))
# print(fetch_closes(['AAPL'], start = "2025-1-10", end = "2025-3-10"))
# print(fetch_closes(['GOOG'], start = "2025-1-10", end = "2025-3-10"))


def simple_returns(prices):
    return prices.pct_change().dropna()

def cum_return_from_returns(returns):
    return (1 + returns).cumprod() - 1

def annualize_mean_std(ret_series, periods=252):
    mean = ret_series.mean() * periods
    std = ret_series.std() * np.sqrt(periods)
    return mean, std

def max_drawdown(price_series):
    roll_max = price_series.cummax()
    drawdown = (price_series - roll_max) / roll_max
    dd = drawdown.min()
    # find start and end dates of that drawdown
    end = drawdown.idxmin()
    start = price_series[:end].idxmax()
    return dd, start, end

def metrics_in_window(prices, returns, ticker, start, end):
  s = start; e = end
  price_w = prices.loc[s:e, ticker]
  ret_w = returns.loc[s:e, ticker]
  cumr = float((1 + ret_w).prod() - 1) if not ret_w.empty else np.nan
  ann_mean, ann_std = annualize_mean_std(ret_w.dropna())
  sharpe = ann_mean / ann_std if ann_std!=0 else np.nan
  dd, dd_start, dd_end = max_drawdown(price_w) if len(price_w)>0 else (np.nan,None,None)
  return dict(cumulative_return=cumr, annual_return=ann_mean, annual_vol=ann_std,
              sharpe=sharpe, max_drawdown=dd, dd_start=dd_start, dd_end=dd_end,
              n_days = (price_w.index[-1]-price_w.index[0]).days+1 if len(price_w)>0 else 0)



start_date = datetime(2017, 1, 1)
end_date =datetime(2025, 11, 20)

five_year_bins = []
current_bin_start = start_date

while current_bin_start <= end_date:
    current_bin_end = current_bin_start + relativedelta(years=5) - relativedelta(days=1) # End of the 5-year period

    # Adjust the end date of the last bin if it exceeds the overall end_date
    if current_bin_end > end_date:
        current_bin_end = end_date

    five_year_bins.append((current_bin_start, current_bin_end))
    current_bin_start = current_bin_start + relativedelta(years=5)

# Print the generated bins
#
drawdown_events = []
for bin in five_year_bins:
  closes = fetch_closes(tickers, bin[0], bin[1])
  drawdown_events.append(max_drawdown(closes['Close_^GSPC']))

"""Find max_dropdown period for snp 500 in 2004-2025, we can also make 5 year bins and get the max_drawdown in those bins.

Covid outbreak timeline: Jan 2020 - August 2021
In February and March 2020, as the pandemic spread globally, stock markets experienced massive downturns. The S&P 500 index fell by over 34% from its peak in February to its trough on March 23, the fastest decline into a bear market in history. This volatility was historically unprecedented for an infectious disease outbreak, primarily driven by economic uncertainty, national lockdowns, and widespread travel restrictions.
"""

import pandas as pd
import numpy as np
import scipy.stats as stats # Added import for statistical tests
import seaborn as sns
# --- Inputs (editable) ----------------------------------------------------
tickers = ["^GSPC", "GLD"]
# Use broad span to cover all events plus context
global_start = "2010-01-01"
global_end   = "2025-11-26"

# User-supplied event list (accepts single date strings or "YYYY-MM-DD:YYYY-MM-DD" ranges)
raw_events = [               # Dot-com peak (use as single-day reference)
    "2007-10-11:2008-09-16",    # 2007-2008 financial crisis period (user's range)
    "2025-01-03:2025-05-04" ,    # User-supplied 2025 tariff event
    "2020-01-20:2021-08-31" ,#covid lockdown era
    "2020-01-20:2025-11-30"  #from covid till now
]

# window padding (days) to analyze around single-date events
pad_days = 60
# baseline length used for significance test (days before event)
baseline_days = 252  # ~1 trading year
# The following lines caused the DateParseError as they tried to convert a date range string directly.
# pd.to_datetime(raw_events[2].split(":")[1])
# print(pd.to_datetime(raw_events[1]) - pd.Timedelta(days=pad_days))
# print(pd.to_datetime(raw_events[1]) + pd.Timedelta(days=pad_days))

events = []
for r in raw_events:
  if ":" in r:
      a, b = r.split(":")
      start = pd.to_datetime(a)
      end = pd.to_datetime(b)
  else:
      center = pd.to_datetime(r)
      start = center - pd.Timedelta(days=pad_days)
      end = center + pd.Timedelta(days=pad_days)

  events.append((r, start, end))

for e in drawdown_events:
  print(e)
  print(e[1])
  print(e[2])
  events.append((e[0], e[1], e[2]))


print(events)



def plot_cum_returns(tickers, start, end):
  closes = fetch_closes(tickers, start, end)
  returns = simple_returns(closes)
  cum_returns = cum_return_from_returns(returns)
  annualized_stats =  annualize_mean_std(simple_returns(closes), 252)
  plt.figure(figsize=(12,5))
  for col in cum_returns.columns:
      plt.plot(cum_returns.index, cum_returns[col], label=col)
  plt.title("Cumulative Returns")
  plt.legend()
  plt.show()

  # rolling volatility (30 trading days)
  rolling_vol = returns.rolling(window=30).std() * (252**0.5)
  plt.figure(figsize=(12,5))
  for col in rolling_vol.columns:
      plt.plot(rolling_vol.index, rolling_vol[col], label=col)
  plt.title("30-day Rolling Annualized Volatility")
  plt.legend()
  plt.show()

  # correlation heatmap of returns
  plt.figure(figsize=(6,5))
  sns.heatmap(returns.corr(), annot=True, fmt=".2f", cmap="vlag", center=0)
  plt.title("Return Correlation")
  plt.show()

def comapre_with_portfolio(portfolio_tickers, weights, ticker2, start, end):
  if isinstance(portfolio_tickers, str):
        portfolio_tickers = [portfolio_tickers]
  portfolio_weights = np.array(weights, dtype=float)
  if len(portfolio_weights) != len(portfolio_tickers):
      raise ValueError("portfolio_weights must have same length as portfolio_tickers")

  closes_port = fetch_closes_sum(portfolio_tickers, weights, start, end)
  closes = fetch_closes(tickers, start, end)

  if hasattr(closes_port, "columns") and len(closes_port.columns) == 1:
        closes_port = closes_port.iloc[:, 0]

  returns_port = simple_returns(closes_port)
  returns = simple_returns(closes)
  cum_returns_port = cum_return_from_returns(returns_port.dropna())
  cum_returns = cum_return_from_returns(returns)

  annualized_stats_port =  annualize_mean_std(closes_port, 252)
  annualized_stat       =  annualize_mean_std(closes, 252)
  plt.figure(figsize=(12, 5))
    # plot each comparison ticker
  for col in cum_returns.columns:
      plt.plot(cum_returns.index, cum_returns[col], label=col)

  plt.plot(cum_returns_port.index, cum_returns_port.values, label="Portfolio", linewidth=2, linestyle="--")


  # plot portfolio (Series)
  plt.title("Cumulative Returns")
  plt.xlabel("Date")
  plt.ylabel("Cumulative return")
  plt.legend()
  plt.grid(alpha=0.2)
  plt.show()

  all_returns = pd.concat([returns, returns_port], axis= 1)
  print(all_returns)
  sns.heatmap(all_returns.corr(), annot=True, fmt=".2f", cmap="vlag", center=0)
  plt.title("Return Correlation")
  plt.show()

for e in events[0:4]:
  start = (e[1])
  end = (e[2])
  plot_cum_returns(tickers, start, end)

for e in events[4:]:
  start = (e[1])
  end = (e[2])
  plot_cum_returns(tickers, start, end)

"""Compare with my portfolio."""

tickers = ["GLD", "^GSPC"]
comapre_with_portfolio(['AAPL', 'JPM', 'MSFT', 'NVDA', "VOO"], [0.2, 0.0183, 0.053, 0.31, 0.254], tickers, "2023-01-01", "2025-11-28" )

"""Compare 3 year annualized returns for "GLD", "GDX", "GDXJ" and "SNP500" or any portfolio

"""

def annualized_return(prices, years=3):
    df = {}

    # get actual start and end dates of the DataFrame
    start_date = prices.index[0]
    end_date   = prices.index[-1]

    for col in prices.columns:
        price_start = prices[col].iloc[0]
        price_end   = prices[col].iloc[-1]

        ann_ret = (price_end / price_start)**(1/years) - 1
        df[col] = ann_ret

    # build final df
    df_final = pd.DataFrame(df, index=[f"{start_date.strftime('%y-%m-%d')} - {end_date.strftime('%y-%m-%d')}"])
    return df_final

def get_avg_for(tickers, start_date = datetime(2014, 1, 1), end_date = datetime(2025, 11, 25)):
  three_year_bins = []
  current_bin_start = start_date

  while current_bin_start <= end_date:
      current_bin_end = current_bin_start + relativedelta(years=3) - relativedelta(days=1) # End of the 3-year period

      # Adjust the end date of the last bin if it exceeds the overall end_date
      if current_bin_end > end_date:
          current_bin_end = end_date

      three_year_bins.append(
      f"{current_bin_start.strftime('%y-%m-%d')}:{current_bin_end.strftime('%y-%m-%d')}")

      current_bin_start = current_bin_start + relativedelta(years=3)
  three_year_ann_average_df = pd.DataFrame()
  for bin in three_year_bins:
      closes = fetch_closes(tickers, f'20{bin.split(":")[0]}', f'20{bin.split(":")[1]}')
      row_df = annualized_return(closes)
      three_year_ann_average_df = pd.concat([three_year_ann_average_df, row_df])
  return three_year_ann_average_df

def plot_grouped_cagrs(cagr_df, figsize=(12,5), rot=30):
    n_bins, n_tickers = len(cagr_df), len(cagr_df.columns)
    x = np.arange(n_bins)
    width = 0.8 / max(1, n_tickers)
    plt.figure(figsize=figsize)
    for i, col in enumerate(cagr_df.columns):
        plt.bar(x + i*width, cagr_df[col].values, width=width, label=col)
    plt.xticks(x + (n_tickers-1)*width/2, cagr_df.index, rotation=rot, ha="right")
    plt.ylabel("3-year annualized return (CAGR)")
    plt.title("3-year annualized returns by ticker")
    plt.legend(ncol=1, bbox_to_anchor=(1.02, 1), loc="upper left")
    plt.grid(axis="y", alpha=0.2)
    plt.tight_layout()
    plt.show()

plt.figure(figsize=(12, 5))
tickers = ['GLD', "^GSPC", "GDX", "GDXJ", "NEM", "^RUT"]
k = get_avg_for(tickers)
# print(k)
# for i in k.columns:
#   # print(f'Close_{i}')
#   plt.bar(k.index, k[i])

plot_grouped_cagrs(k)

fetch_closes(tickers, "2020-1-1", "2022-1-1").tail()

